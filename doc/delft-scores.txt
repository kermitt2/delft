********************************************************
****** CoNLL-2003 **************************************
********************************************************


**** BidLSTM-CRF ***************************************
(Lample and al., 2016)


default hyper-parameters
---

****** Recurrent Dropout to 0.5 *******

average over 10 folds
	macro f1 = 0.9069695811384205
	macro precision = 0.9051709269444954
	macro recall = 0.9087818696883854 


** Worst ** model scores - 

             precision    recall  f1-score   support

        ORG     0.8877    0.8657    0.8766      1661
        LOC     0.9086    0.9353    0.9217      1668
       MISC     0.7905    0.8276    0.8086       702
        PER     0.9521    0.9586    0.9553      1617

avg / total     0.8998    0.9081    0.9039      5648


** Best ** model scores - 

             precision    recall  f1-score   support

        ORG     0.8877    0.8856    0.8867      1661
        LOC     0.9309    0.9371    0.9340      1668
       MISC     0.8279    0.7949    0.8110       702
        PER     0.9571    0.9647    0.9609      1617

avg / total     0.9135    0.9122    0.9128      5648


********************************
with ELMo **********************
********************************

average over 10 folds
	macro f1 = 0.9227116560737748
	macro precision = 0.9207727798287024
	macro recall = 0.924663597733711 


** Worst ** model scores - 

                  precision    recall  f1-score   support

            MISC     0.8146    0.8134    0.8140       702
             LOC     0.9319    0.9353    0.9336      1668
             ORG     0.9025    0.8970    0.8998      1661
             PER     0.9692    0.9728    0.9710      1617

all (micro avg.)     0.9195    0.9196    0.9195      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

            MISC     0.8255    0.8291    0.8273       702
             LOC     0.9391    0.9430    0.9411      1668
             ORG     0.9053    0.9151    0.9102      1661
             PER     0.9745    0.9703    0.9724      1617

all (micro avg.)     0.9250    0.9285    0.9267      5648





**** TRAINING WITH VALIDATION SET ***********************************

25 epochs

average over 10 folds
	macro f1 = 0.910963634661394
	macro precision = 0.9081389480949665
	macro recall = 0.9138101983002833 


** Worst ** model scores - 

             precision    recall  f1-score   support

        ORG     0.8860    0.8886    0.8873      1661
        PER     0.9569    0.9474    0.9521      1617
        LOC     0.9104    0.9382    0.9241      1668
       MISC     0.8126    0.8091    0.8108       702

avg / total     0.9043    0.9102    0.9073      5648


** Best ** model scores - 

             precision    recall  f1-score   support

        ORG     0.8816    0.9007    0.8910      1661
        PER     0.9569    0.9610    0.9590      1617
        LOC     0.9300    0.9394    0.9347      1668
       MISC     0.8324    0.8134    0.8228       702

avg / total     0.9115    0.9186    0.9150      5648

---

30 epochs

average over 10 folds
	macro f1 = 0.9110954589012982
	macro precision = 0.9083825975530369
	macro recall = 0.9138279036827196 


** Worst ** model scores - 

             precision    recall  f1-score   support

        ORG     0.8844    0.8892    0.8868      1661
        LOC     0.9173    0.9311    0.9241      1668
       MISC     0.8025    0.8105    0.8065       702
        PER     0.9556    0.9573    0.9564      1617

avg / total     0.9043    0.9113    0.9078      5648


** Best ** model scores - 

             precision    recall  f1-score   support

        ORG     0.8994    0.8880    0.8937      1661
        LOC     0.9175    0.9406    0.9290      1668
       MISC     0.8161    0.8219    0.8190       702
        PER     0.9606    0.9641    0.9623      1617

avg / total     0.9120    0.9171    0.9145      5648

---

recurrent-dropout at .20 (was .25)


average over 10 folds
	macro f1 = 0.9109276625596973
	macro precision = 0.9088374086457675
	macro recall = 0.9130311614730878 


** Worst ** model scores - 

             precision    recall  f1-score   support

        PER     0.9585    0.9579    0.9582      1617
        LOC     0.9138    0.9347    0.9241      1668
        ORG     0.8819    0.8814    0.8817      1661
       MISC     0.8105    0.7920    0.8012       702

avg / total     0.9047    0.9079    0.9063      5648


** Best ** model scores - 

             precision    recall  f1-score   support

        PER     0.9530    0.9654    0.9591      1617
        LOC     0.9288    0.9382    0.9335      1668
        ORG     0.8917    0.8922    0.8920      1661
       MISC     0.8293    0.8234    0.8263       702

avg / total     0.9127    0.9182    0.9154      5648

---

recurrent-dropout at .15 (was .25)


average over 10 folds
	macro f1 = 0.911356473365748
	macro precision = 0.9090619451383816
	macro recall = 0.913668555240793 


** Worst ** model scores - 

             precision    recall  f1-score   support

       MISC     0.8049    0.7934    0.7991       702
        PER     0.9482    0.9518    0.9500      1617
        LOC     0.9226    0.9365    0.9295      1668
        ORG     0.8785    0.8922    0.8853      1661

avg / total     0.9025    0.9101    0.9063      5648


** Best ** model scores - 

             precision    recall  f1-score   support

       MISC     0.8277    0.8077    0.8176       702
        PER     0.9594    0.9635    0.9614      1617
        LOC     0.9219    0.9418    0.9318      1668
        ORG     0.9029    0.8904    0.8966      1661

avg / total     0.9158    0.9163    0.9160      5648


---

recurrent-dropout at .1 (was .25)


average over 10 folds
	macro f1 = 0.91023595205431
	macro precision = 0.9078993366389225
	macro recall = 0.9125885269121813 


** Worst ** model scores - 

             precision    recall  f1-score   support

        LOC     0.9148    0.9329    0.9237      1668
       MISC     0.8234    0.7906    0.8067       702
        ORG     0.8810    0.8784    0.8797      1661
        PER     0.9417    0.9598    0.9507      1617

avg / total     0.9019    0.9069    0.9044      5648


** Best ** model scores - 

             precision    recall  f1-score   support

        LOC     0.9243    0.9371    0.9306      1668
       MISC     0.8188    0.8177    0.8182       702
        ORG     0.8944    0.8922    0.8933      1661
        PER     0.9525    0.9672    0.9598      1617

avg / total     0.9107    0.9177    0.9142      5648





----------------------------
**** BidGRU-CRF ***********************************
(no ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.9038481587859358
	macro precision = 0.8996194112986517
	macro recall = 0.9081267705382438 


** Worst ** model scores - 

             precision    recall  f1-score   support

        LOC     0.8867    0.9388    0.9121      1668
        ORG     0.8750    0.8639    0.8694      1661
       MISC     0.8316    0.7806    0.8053       702
        PER     0.9411    0.9592    0.9501      1617

avg / total     0.8927    0.9030    0.8978      5648


** Best ** model scores - 

             precision    recall  f1-score   support

        LOC     0.9123    0.9353    0.9236      1668
        ORG     0.8843    0.8790    0.8816      1661
       MISC     0.8235    0.7977    0.8104       702
        PER     0.9528    0.9617    0.9572      1617

avg / total     0.9052    0.9092    0.9072      5648


----------------------------
**** BidGRU-CRF ***********************************
(with ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.9203256867927602
	macro precision = 0.9169219089704612
	macro recall = 0.9237606232294617 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8936    0.9001    0.8968      1661
             PER     0.9668    0.9734    0.9701      1617
             LOC     0.9260    0.9376    0.9318      1668
            MISC     0.8000    0.8205    0.8101       702

all (micro avg.)     0.9123    0.9223    0.9172      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.9060    0.9115    0.9088      1661
             PER     0.9788    0.9728    0.9758      1617
             LOC     0.9360    0.9376    0.9368      1668
            MISC     0.8099    0.8191    0.8144       702

all (micro avg.)     0.9235    0.9253    0.9244      5648


----------------------------
**** BidGRU-CRF ***********************************
(with validation set, no ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.9028025431655177
	macro precision = 0.8982200750538265
	macro recall = 0.9074362606232296 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8834    0.8531    0.8680      1661
             LOC     0.9062    0.9388    0.9223      1668
             PER     0.9299    0.9678    0.9485      1617
            MISC     0.8072    0.7934    0.8003       702

all (micro avg.)     0.8948    0.9039    0.8993      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8770    0.8844    0.8807      1661
             LOC     0.9261    0.9317    0.9289      1668
             PER     0.9550    0.9592    0.9571      1617
            MISC     0.7989    0.8034    0.8011       702

all (micro avg.)     0.9041    0.9097    0.9069      5648



----------------------------
**** BidGRU-CRF ***********************************
(with validation set, with ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.9243585166451517
	macro precision = 0.9211930849572475
	macro recall = 0.9275495750708215 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9680    0.9740    0.9710      1617
             ORG     0.9036    0.9085    0.9060      1661
             LOC     0.9301    0.9406    0.9353      1668
            MISC     0.8089    0.8319    0.8202       702

all (micro avg.)     0.9178    0.9272    0.9225      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9735    0.9759    0.9747      1617
             ORG     0.8971    0.9235    0.9101      1661
             LOC     0.9449    0.9353    0.9400      1668
            MISC     0.8246    0.8305    0.8275       702

all (micro avg.)     0.9237    0.9304    0.9271      5648



----------------------------
BiLSTM-CNN-CRF (no ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.9073610377569482
	macro precision = 0.9025451669268871
	macro recall = 0.9122344192634563 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9548    0.9530    0.9539      1617
             LOC     0.9079    0.9400    0.9237      1668
             ORG     0.8618    0.8934    0.8773      1661
            MISC     0.8138    0.7906    0.8020       702

all (micro avg.)     0.8961    0.9115    0.9037      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9643    0.9518    0.9580      1617
             LOC     0.9179    0.9388    0.9283      1668
             ORG     0.8738    0.9007    0.8870      1661
            MISC     0.8376    0.7934    0.8149       702

all (micro avg.)     0.9083    0.9132    0.9107      5648


----------------------------
BiLSTM-CNN-CRF (no ELMo)
----------------------------
train with validation set
----------------------------

average over 10 folds
	macro f1 = 0.9101551430232169
	macro precision = 0.9063440210138378
	macro recall = 0.9140049575070822 


** Worst ** model scores - 

                  precision    recall  f1-score   support

            MISC     0.8082    0.8105    0.8094       702
             PER     0.9496    0.9561    0.9529      1617
             ORG     0.8774    0.8874    0.8824      1661
             LOC     0.9152    0.9376    0.9263      1668

all (micro avg.)     0.9007    0.9124    0.9065      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

            MISC     0.8205    0.8077    0.8141       702
             PER     0.9617    0.9623    0.9620      1617
             ORG     0.8970    0.8910    0.8940      1661
             LOC     0.9090    0.9400    0.9243      1668

all (micro avg.)     0.9097    0.9155    0.9126      5648


----------------------------
BiLSTM-CNN-CRF (with ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.9229714804031515
	macro precision = 0.9201331319950187
	macro recall = 0.9258321529745043 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9740    0.9716    0.9728      1617
             ORG     0.8737    0.9037    0.8884      1661
             LOC     0.9392    0.9347    0.9369      1668
            MISC     0.8126    0.8091    0.8108       702

all (micro avg.)     0.9137    0.9205    0.9171      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9758    0.9716    0.9737      1617
             ORG     0.8958    0.9157    0.9056      1661
             LOC     0.9380    0.9430    0.9405      1668
            MISC     0.8212    0.8376    0.8293       702

all (micro avg.)     0.9214    0.9301    0.9257      5648



----------------------------
BiLSTM-CNN-CRF (with ELMo)
----------------------------
train with validation set
----------------------------

average over 10 folds
	macro f1 = 0.9267886466836008
	macro precision = 0.9236787695219577
	macro recall = 0.9299220963172804 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             LOC     0.9388    0.9382    0.9385      1668
            MISC     0.8031    0.8191    0.8110       702
             ORG     0.9005    0.9157    0.9081      1661
             PER     0.9770    0.9722    0.9746      1617

all (micro avg.)     0.9211    0.9265    0.9238      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             LOC     0.9304    0.9460    0.9382      1668
            MISC     0.8208    0.8547    0.8374       702
             ORG     0.9200    0.9139    0.9169      1661
             PER     0.9807    0.9740    0.9774      1617

all (micro avg.)     0.9275    0.9333    0.9304      5648


--------------------------------
BiLSTM-CRF-CASING (without ELMo)
--------------------------------

average over 10 folds
	macro f1 = 0.9072633045005467
	macro precision = 0.9057544135162671
	macro recall = 0.9087818696883853 


** Worst ** model scores - 

                  precision    recall  f1-score   support

            MISC     0.8072    0.7991    0.8031       702
             ORG     0.8851    0.8718    0.8784      1661
             LOC     0.9078    0.9388    0.9231      1668
             PER     0.9578    0.9555    0.9567      1617

all (micro avg.)     0.9032    0.9065    0.9048      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

            MISC     0.8321    0.7835    0.8070       702
             ORG     0.8888    0.8850    0.8869      1661
             LOC     0.9158    0.9388    0.9272      1668
             PER     0.9507    0.9666    0.9586      1617

all (micro avg.)     0.9083    0.9117    0.9100      5648


-----------------------------
BiLSTM-CRF-CASING (with ELMo)
-----------------------------

average over 10 folds
	macro f1 = 0.9240220967487194
	macro precision = 0.9235466594908284
	macro recall = 0.9245042492917847 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9698    0.9716    0.9707      1617
            MISC     0.7942    0.8134    0.8037       702
             LOC     0.9313    0.9424    0.9368      1668
             ORG     0.9096    0.9031    0.9063      1661

all (micro avg.)     0.9186    0.9232    0.9209      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9782    0.9722    0.9752      1617
            MISC     0.8364    0.8376    0.8370       702
             LOC     0.9337    0.9376    0.9357      1668
             ORG     0.9114    0.9169    0.9142      1661

all (micro avg.)     0.9277    0.9290    0.9283      5648



----------------------------
BiLSTM-CNN (CASING, no ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.8923685257762012
	macro precision = 0.8813521248524246
	macro recall = 0.9036650141643058 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8260    0.8802    0.8522      1661
             LOC     0.8928    0.9287    0.9104      1668
            MISC     0.8003    0.7707    0.7852       702
             PER     0.9511    0.9493    0.9502      1617

all (micro avg.)     0.8778    0.9007    0.8891      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8357    0.8820    0.8582      1661
             LOC     0.9094    0.9269    0.9181      1668
            MISC     0.7869    0.7892    0.7881       702
             PER     0.9532    0.9573    0.9553      1617

all (micro avg.)     0.8844    0.9053    0.8947      5648


------------------------------
BiLSTM-CNN (CASING, with ELMo)
------------------------------

average over 10 folds
	macro f1 = 0.9166408580816073
	macro precision = 0.9095803428961048
	macro recall = 0.9238137393767705 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8725    0.9019    0.8869      1661
             PER     0.9740    0.9722    0.9731      1617
             LOC     0.9227    0.9376    0.9301      1668
            MISC     0.8006    0.8177    0.8090       702

all (micro avg.)     0.9068    0.9221    0.9144      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8795    0.9139    0.8964      1661
             PER     0.9698    0.9716    0.9707      1617
             LOC     0.9315    0.9376    0.9346      1668
            MISC     0.8155    0.8376    0.8264       702

all (micro avg.)     0.9121    0.9279    0.9200      5648


-------------------------------------------------------
BiLSTM-CNN (CASING, no ELMo, train with validation set)
-------------------------------------------------------

average over 10 folds
	macro f1 = 0.8935373160803783
	macro precision = 0.8831134985078204
	macro recall = 0.9042138810198301 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8491    0.8706    0.8597      1661
             PER     0.9449    0.9549    0.9499      1617
             LOC     0.8851    0.9371    0.9103      1668
            MISC     0.7956    0.7650    0.7800       702

all (micro avg.)     0.8809    0.9012    0.8910      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8515    0.8838    0.8674      1661
             PER     0.9531    0.9542    0.9536      1617
             LOC     0.9049    0.9353    0.9198      1668
            MISC     0.8038    0.7877    0.7957       702

all (micro avg.)     0.8904    0.9072    0.8987      5648



---------------------------------------------------------
BiLSTM-CNN (CASING, with ELMo, train with validation set)
---------------------------------------------------------

average over 10 folds
	macro f1 = 0.9201250430609109
	macro precision = 0.913559893189429
	macro recall = 0.9267882436260624 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8880    0.9067    0.8972      1661
            MISC     0.8044    0.8319    0.8179       702
             PER     0.9734    0.9734    0.9734      1617
             LOC     0.9240    0.9329    0.9284      1668

all (micro avg.)     0.9121    0.9242    0.9181      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8788    0.9211    0.8995      1661
            MISC     0.8123    0.8262    0.8192       702
             PER     0.9747    0.9759    0.9753      1617
             LOC     0.9342    0.9371    0.9356      1668

all (micro avg.)     0.9137    0.9297    0.9216      5648




---------------------------------------------------------
BiLSTM-CRF (with BERT cased model only, train without validation set)
---------------------------------------------------------

training runtime: 18353.987 seconds 

Evaluation on test set:
  f1 (micro): 90.23
                  precision    recall  f1-score   support

             PER     0.9639    0.9573    0.9606      1617
             ORG     0.8572    0.8964    0.8764      1661
             LOC     0.9309    0.9041    0.9173      1668
            MISC     0.8233    0.7635    0.7923       702

all (micro avg.)     0.9051    0.8996    0.9023      5648


---------------------------------------------------------
BiLSTM-CRF (with BERT cased model + GLOVE, train without validation set)
---------------------------------------------------------

training runtime: 27595.533 seconds 

Evaluation on test set:
  f1 (micro): 92.02
                  precision    recall  f1-score   support

             LOC     0.9344    0.9400    0.9372      1668
             ORG     0.8854    0.9115    0.8982      1661
             PER     0.9715    0.9709    0.9712      1617
            MISC     0.8196    0.8091    0.8143       702

all (micro avg.)     0.9163    0.9242    0.9202      5648



---------------------------------------------------------
BERT bert-base-en (cased), CoNLL 2003 NER (train without validation set)
---------------------------------------------------------

Average over 10 folds
                  precision    recall  f1-score   support

             LOC     0.9242    0.9309    0.9275      1668
            MISC     0.7674    0.8226    0.7941       702
             ORG     0.8790    0.9016    0.8902      1661
             PER     0.9573    0.9561    0.9567      1617

  macro f1 = 0.9077 
  macro precision = 0.8995
  macro recall = 0.9160 

** Worst ** model scores - run 3
                  precision    recall  f1-score   support

             LOC     0.9239    0.9317    0.9278      1668
            MISC     0.7583    0.8177    0.7868       702
             ORG     0.8725    0.8983    0.8852      1661
             PER     0.9564    0.9499    0.9531      1617

all (micro avg.)     0.8959    0.9129    0.9043      5648


** Best ** model scores - run 4
                  precision    recall  f1-score   support

             LOC     0.9253    0.9353    0.9302      1668
            MISC     0.7766    0.8319    0.8033       702
             ORG     0.8776    0.9061    0.8916      1661
             PER     0.9640    0.9592    0.9616      1617

all (micro avg.)     0.9025    0.9207    0.9115      5648

----------------------------------------------------------------------



---------------------------------------------------------
BERT-CRF bert-base-en (cased), CoNLL 2003 NER (train without validation set)
---------------------------------------------------------

Average over 10 folds
            precision    recall  f1-score   support

       ORG     0.8804    0.9114    0.8957      1661
      MISC     0.7823    0.8189    0.8002       702
       PER     0.9633    0.9576    0.9605      1617
       LOC     0.9290    0.9316    0.9303      1668

  macro f1 = 0.9120
  macro precision = 0.9050
  macro recall = 0.9191 


** Worst ** model scores - 9
                  precision    recall  f1-score   support

             ORG     0.8736    0.9073    0.8901      1661
             PER     0.9596    0.9555    0.9575      1617
             LOC     0.9221    0.9293    0.9256      1668
            MISC     0.7757    0.8177    0.7961       702

all (micro avg.)     0.8992    0.9164    0.9078      5648


** Best ** model scores - 2
                  precision    recall  f1-score   support

             ORG     0.8897    0.9229    0.9060      1661
             PER     0.9627    0.9573    0.9600      1617
             LOC     0.9375    0.9353    0.9364      1668
            MISC     0.7862    0.8120    0.7989       702

all (micro avg.)     0.9110    0.9226    0.9168      5648







********************************************************
****** Ontonotes 5.0 ***********************************
********************************************************


****** all corpus ***********************************

nb total sentences: 103766
nb total tokens: 2192130
84050 train sequences
	 nb. tokens 1772848
9339 validation sequences
	 nb. tokens 198565
10377 evaluation sequences
	 nb. tokens 220717


82122 train sequences
	 nb. tokens 1633660
	 with nb. entities 239617
12678 validation sequences
	 nb. tokens 249271
	 with nb. entities 37534
8262 evaluation sequences
	 nb. tokens 152728
	 with nb. entities 20913
total distinct characters: 114


------ without ELMo ---------------------------------------

    training runtime: 7969.122 seconds 

Evaluation on test set:
  f1 (micro): 87.01
                  precision    recall  f1-score   support

            DATE     0.8029    0.8695    0.8349      1602
        CARDINAL     0.8130    0.8139    0.8135       935
          PERSON     0.9061    0.9371    0.9214      1988
             GPE     0.9617    0.9411    0.9513      2240
             ORG     0.8799    0.8568    0.8682      1795
           MONEY     0.8903    0.8790    0.8846       314
            NORP     0.9226    0.9501    0.9361       841
         ORDINAL     0.7873    0.8923    0.8365       195
            TIME     0.5772    0.6698    0.6201       212
     WORK_OF_ART     0.6000    0.5060    0.5490       166
             LOC     0.7340    0.7709    0.7520       179
           EVENT     0.5000    0.5556    0.5263        63
         PRODUCT     0.6528    0.6184    0.6351        76
         PERCENT     0.8717    0.8567    0.8642       349
        QUANTITY     0.7155    0.7905    0.7511       105
             FAC     0.7167    0.6370    0.6745       135
        LANGUAGE     0.8462    0.5000    0.6286        22
             LAW     0.7308    0.4750    0.5758        40

all (micro avg.)     0.8647    0.8755    0.8701     11257




------ with ELMo ------------------------------------------
(usual model 5.5B)

Evaluation on test set:
	f1 (micro): 79.62
             precision    recall  f1-score   support

WORK_OF_ART     0.5510    0.6506    0.5967       166
    PRODUCT     0.6582    0.6842    0.6710        76
      MONEY     0.8116    0.8503    0.8305       314
        FAC     0.7130    0.5704    0.6337       135
   LANGUAGE     0.7778    0.6364    0.7000        22
   QUANTITY     0.1361    0.8000    0.2327       105
       TIME     0.6370    0.4387    0.5196       212
        GPE     0.9535    0.9437    0.9486      2240
      EVENT     0.6316    0.7619    0.6906        63
    PERCENT     0.8499    0.8596    0.8547       349
        ORG     0.9003    0.8758    0.8879      1795
        LOC     0.7611    0.7654    0.7632       179
     PERSON     0.9297    0.9452    0.9374      1988
    ORDINAL     0.8148    0.1128    0.1982       195
        LAW     0.5405    0.5000    0.5195        40
       NORP     0.9191    0.9322    0.9256       841
   CARDINAL     0.8512    0.1102    0.1951       935
       DATE     0.8537    0.5137    0.6415      1602

avg / total     0.8423    0.7548    0.7962     11257



------ with ELMo ------------------------------------------
ELMo first model (not the 5.5B)

average over 10 folds
	macro f1 = 0.7852888179838143
	macro precision = 0.8304988758759713
	macro recall = 0.744754375055521 


** Best ** model scores - 

                  precision    recall  f1-score   support

           MONEY     0.8431    0.8726    0.8576       314
            TIME     0.6692    0.4104    0.5088       212
           EVENT     0.4889    0.6984    0.5752        63
             FAC     0.7736    0.6074    0.6805       135
          PERSON     0.9129    0.9432    0.9278      1988
             GPE     0.9549    0.9545    0.9547      2240
             LOC     0.7391    0.7598    0.7493       179
         ORDINAL     0.8704    0.2410    0.3775       195
     WORK_OF_ART     0.6294    0.6446    0.6369       166
             ORG     0.8883    0.8641    0.8760      1795
        LANGUAGE     0.7500    0.6818    0.7143        22
         PERCENT     0.8097    0.8166    0.8131       349
            DATE     0.8392    0.5181    0.6407      1602
        QUANTITY     0.1434    0.7714    0.2418       105
        CARDINAL     0.7789    0.1658    0.2734       935
            NORP     0.8930    0.8930    0.8930       841
             LAW     0.5000    0.4500    0.4737        40
         PRODUCT     0.6250    0.7237    0.6707        76

all (micro avg.)     0.8374    0.7578    0.7956     11257


------

batch_size == 120 (instead of 20, which was the problem above)

Evaluation on test set:
  f1 (micro): 89.01
                  precision    recall  f1-score   support

             LAW     0.7188    0.5750    0.6389        40
         PERCENT     0.8946    0.8997    0.8971       349
           EVENT     0.6212    0.6508    0.6357        63
        CARDINAL     0.8616    0.7722    0.8144       935
        QUANTITY     0.7838    0.8286    0.8056       105
            NORP     0.9232    0.9572    0.9399       841
             LOC     0.7459    0.7709    0.7582       179
            DATE     0.8629    0.8252    0.8437      1602
        LANGUAGE     0.8750    0.6364    0.7368        22
             GPE     0.9637    0.9607    0.9622      2240
         ORDINAL     0.8145    0.9231    0.8654       195
             ORG     0.9033    0.8903    0.8967      1795
           MONEY     0.8851    0.9076    0.8962       314
             FAC     0.8257    0.6667    0.7377       135
            TIME     0.6592    0.6934    0.6759       212
          PERSON     0.9350    0.9477    0.9413      1988
     WORK_OF_ART     0.6467    0.7169    0.6800       166
         PRODUCT     0.6867    0.7500    0.7170        76

all (micro avg.)     0.8939    0.8864    0.8901     11257





------------------------------------------------------------------------
Le Monde corpus FTB

wiki-fr (fasttext)

average over 10 folds
  macro f1 = 0.9100881012386587
  macro precision = 0.9048633201198737
  macro recall = 0.9153907496012759 

** Worst ** model scores - 

                  precision    recall  f1-score   support

      <location>     0.9467    0.9647    0.9556       368
   <institution>     0.8621    0.8333    0.8475        30
      <artifact>     1.0000    0.5000    0.6667         4
  <organisation>     0.9146    0.8089    0.8585       225
        <person>     0.9264    0.9522    0.9391       251
      <business>     0.8463    0.8936    0.8693       376

all (micro avg.)     0.9040    0.9083    0.9061      1254

** Best ** model scores - 

                  precision    recall  f1-score   support

      <location>     0.9439    0.9592    0.9515       368
   <institution>     0.8667    0.8667    0.8667        30
      <artifact>     1.0000    0.5000    0.6667         4
  <organisation>     0.8813    0.8578    0.8694       225
        <person>     0.9453    0.9641    0.9546       251
      <business>     0.8706    0.9122    0.8909       376

all (micro avg.)     0.9090    0.9242    0.9166      1254




with frELMo (stacked with wiki.fr)

average over 10 folds
  macro f1 = 0.9209397554337976
  macro precision = 0.91949107960079
  macro recall = 0.9224082934609251 


** Worst ** model scores - 

                  precision    recall  f1-score   support

  <organisation>     0.8704    0.8356    0.8526       225
        <person>     0.9344    0.9641    0.9490       251
      <artifact>     1.0000    0.5000    0.6667         4
      <location>     0.9173    0.9647    0.9404       368
   <institution>     0.8889    0.8000    0.8421        30
      <business>     0.9130    0.8936    0.9032       376

all (micro avg.)     0.9110    0.9147    0.9129      1254


** Best ** model scores - 

                  precision    recall  f1-score   support

  <organisation>     0.9061    0.8578    0.8813       225
        <person>     0.9416    0.9641    0.9528       251
      <artifact>     1.0000    0.5000    0.6667         4
      <location>     0.9570    0.9674    0.9622       368
   <institution>     0.8889    0.8000    0.8421        30
      <business>     0.9016    0.9255    0.9134       376

all (micro avg.)     0.9268    0.9290    0.9279      1254
