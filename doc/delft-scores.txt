********************************************************
****** CoNLL-2003 **************************************
********************************************************


**** BidLSTM-CRF ***************************************
(Lample and al., 2016)


default hyper-parameters
---

****** Recurrent Dropout to 0.5 *******

average over 10 folds
	macro f1 = 0.9069695811384205
	macro precision = 0.9051709269444954
	macro recall = 0.9087818696883854 


** Worst ** model scores - 

             precision    recall  f1-score   support

        ORG     0.8877    0.8657    0.8766      1661
        LOC     0.9086    0.9353    0.9217      1668
       MISC     0.7905    0.8276    0.8086       702
        PER     0.9521    0.9586    0.9553      1617

avg / total     0.8998    0.9081    0.9039      5648


** Best ** model scores - 

             precision    recall  f1-score   support

        ORG     0.8877    0.8856    0.8867      1661
        LOC     0.9309    0.9371    0.9340      1668
       MISC     0.8279    0.7949    0.8110       702
        PER     0.9571    0.9647    0.9609      1617

avg / total     0.9135    0.9122    0.9128      5648


********************************
with ELMo **********************
********************************

average over 10 folds
	macro f1 = 0.9227116560737748
	macro precision = 0.9207727798287024
	macro recall = 0.924663597733711 


** Worst ** model scores - 

                  precision    recall  f1-score   support

            MISC     0.8146    0.8134    0.8140       702
             LOC     0.9319    0.9353    0.9336      1668
             ORG     0.9025    0.8970    0.8998      1661
             PER     0.9692    0.9728    0.9710      1617

all (micro avg.)     0.9195    0.9196    0.9195      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

            MISC     0.8255    0.8291    0.8273       702
             LOC     0.9391    0.9430    0.9411      1668
             ORG     0.9053    0.9151    0.9102      1661
             PER     0.9745    0.9703    0.9724      1617

all (micro avg.)     0.9250    0.9285    0.9267      5648



********************************
with FLAIR *********************
********************************
default hyper-parameters


average over 10 folds
  macro f1 = 0.9195004430649834
  macro precision = 0.9168160478680043
  macro recall = 0.922202549575071 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9720    0.9647    0.9683      1617
            MISC     0.8290    0.8148    0.8218       702
             ORG     0.8868    0.8958    0.8913      1661
             LOC     0.9161    0.9424    0.9291      1668

all (micro avg.)     0.9126    0.9193    0.9159      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9756    0.9654    0.9705      1617
            MISC     0.8143    0.8433    0.8286       702
             ORG     0.9126    0.9049    0.9087      1661
             LOC     0.9278    0.9394    0.9336      1668

all (micro avg.)     0.9223    0.9248    0.9235      5648



average training runtime: 3749.201 seconds (~62.5 minutes)




**** TRAINING WITH VALIDATION SET ***********************************

25 epochs

average over 10 folds
	macro f1 = 0.910963634661394
	macro precision = 0.9081389480949665
	macro recall = 0.9138101983002833 


** Worst ** model scores - 

             precision    recall  f1-score   support

        ORG     0.8860    0.8886    0.8873      1661
        PER     0.9569    0.9474    0.9521      1617
        LOC     0.9104    0.9382    0.9241      1668
       MISC     0.8126    0.8091    0.8108       702

avg / total     0.9043    0.9102    0.9073      5648


** Best ** model scores - 

             precision    recall  f1-score   support

        ORG     0.8816    0.9007    0.8910      1661
        PER     0.9569    0.9610    0.9590      1617
        LOC     0.9300    0.9394    0.9347      1668
       MISC     0.8324    0.8134    0.8228       702

avg / total     0.9115    0.9186    0.9150      5648

---

30 epochs

average over 10 folds
	macro f1 = 0.9110954589012982
	macro precision = 0.9083825975530369
	macro recall = 0.9138279036827196 


** Worst ** model scores - 

             precision    recall  f1-score   support

        ORG     0.8844    0.8892    0.8868      1661
        LOC     0.9173    0.9311    0.9241      1668
       MISC     0.8025    0.8105    0.8065       702
        PER     0.9556    0.9573    0.9564      1617

avg / total     0.9043    0.9113    0.9078      5648


** Best ** model scores - 

             precision    recall  f1-score   support

        ORG     0.8994    0.8880    0.8937      1661
        LOC     0.9175    0.9406    0.9290      1668
       MISC     0.8161    0.8219    0.8190       702
        PER     0.9606    0.9641    0.9623      1617

avg / total     0.9120    0.9171    0.9145      5648

---

recurrent-dropout at .20 (was .25)


average over 10 folds
	macro f1 = 0.9109276625596973
	macro precision = 0.9088374086457675
	macro recall = 0.9130311614730878 


** Worst ** model scores - 

             precision    recall  f1-score   support

        PER     0.9585    0.9579    0.9582      1617
        LOC     0.9138    0.9347    0.9241      1668
        ORG     0.8819    0.8814    0.8817      1661
       MISC     0.8105    0.7920    0.8012       702

avg / total     0.9047    0.9079    0.9063      5648


** Best ** model scores - 

             precision    recall  f1-score   support

        PER     0.9530    0.9654    0.9591      1617
        LOC     0.9288    0.9382    0.9335      1668
        ORG     0.8917    0.8922    0.8920      1661
       MISC     0.8293    0.8234    0.8263       702

avg / total     0.9127    0.9182    0.9154      5648

---

recurrent-dropout at .15 (was .25)


average over 10 folds
	macro f1 = 0.911356473365748
	macro precision = 0.9090619451383816
	macro recall = 0.913668555240793 


** Worst ** model scores - 

             precision    recall  f1-score   support

       MISC     0.8049    0.7934    0.7991       702
        PER     0.9482    0.9518    0.9500      1617
        LOC     0.9226    0.9365    0.9295      1668
        ORG     0.8785    0.8922    0.8853      1661

avg / total     0.9025    0.9101    0.9063      5648


** Best ** model scores - 

             precision    recall  f1-score   support

       MISC     0.8277    0.8077    0.8176       702
        PER     0.9594    0.9635    0.9614      1617
        LOC     0.9219    0.9418    0.9318      1668
        ORG     0.9029    0.8904    0.8966      1661

avg / total     0.9158    0.9163    0.9160      5648


---

recurrent-dropout at .1 (was .25)


average over 10 folds
	macro f1 = 0.91023595205431
	macro precision = 0.9078993366389225
	macro recall = 0.9125885269121813 


** Worst ** model scores - 

             precision    recall  f1-score   support

        LOC     0.9148    0.9329    0.9237      1668
       MISC     0.8234    0.7906    0.8067       702
        ORG     0.8810    0.8784    0.8797      1661
        PER     0.9417    0.9598    0.9507      1617

avg / total     0.9019    0.9069    0.9044      5648


** Best ** model scores - 

             precision    recall  f1-score   support

        LOC     0.9243    0.9371    0.9306      1668
       MISC     0.8188    0.8177    0.8182       702
        ORG     0.8944    0.8922    0.8933      1661
        PER     0.9525    0.9672    0.9598      1617

avg / total     0.9107    0.9177    0.9142      5648





----------------------------
**** BidGRU-CRF ***********************************
(no ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.9038481587859358
	macro precision = 0.8996194112986517
	macro recall = 0.9081267705382438 


** Worst ** model scores - 

             precision    recall  f1-score   support

        LOC     0.8867    0.9388    0.9121      1668
        ORG     0.8750    0.8639    0.8694      1661
       MISC     0.8316    0.7806    0.8053       702
        PER     0.9411    0.9592    0.9501      1617

avg / total     0.8927    0.9030    0.8978      5648


** Best ** model scores - 

             precision    recall  f1-score   support

        LOC     0.9123    0.9353    0.9236      1668
        ORG     0.8843    0.8790    0.8816      1661
       MISC     0.8235    0.7977    0.8104       702
        PER     0.9528    0.9617    0.9572      1617

avg / total     0.9052    0.9092    0.9072      5648


----------------------------
**** BidGRU-CRF ***********************************
(with ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.9203256867927602
	macro precision = 0.9169219089704612
	macro recall = 0.9237606232294617 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8936    0.9001    0.8968      1661
             PER     0.9668    0.9734    0.9701      1617
             LOC     0.9260    0.9376    0.9318      1668
            MISC     0.8000    0.8205    0.8101       702

all (micro avg.)     0.9123    0.9223    0.9172      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.9060    0.9115    0.9088      1661
             PER     0.9788    0.9728    0.9758      1617
             LOC     0.9360    0.9376    0.9368      1668
            MISC     0.8099    0.8191    0.8144       702

all (micro avg.)     0.9235    0.9253    0.9244      5648


----------------------------
**** BidGRU-CRF ***********************************
(with validation set, no ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.9028025431655177
	macro precision = 0.8982200750538265
	macro recall = 0.9074362606232296 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8834    0.8531    0.8680      1661
             LOC     0.9062    0.9388    0.9223      1668
             PER     0.9299    0.9678    0.9485      1617
            MISC     0.8072    0.7934    0.8003       702

all (micro avg.)     0.8948    0.9039    0.8993      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8770    0.8844    0.8807      1661
             LOC     0.9261    0.9317    0.9289      1668
             PER     0.9550    0.9592    0.9571      1617
            MISC     0.7989    0.8034    0.8011       702

all (micro avg.)     0.9041    0.9097    0.9069      5648



----------------------------
**** BidGRU-CRF ***********************************
(with validation set, with ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.9243585166451517
	macro precision = 0.9211930849572475
	macro recall = 0.9275495750708215 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9680    0.9740    0.9710      1617
             ORG     0.9036    0.9085    0.9060      1661
             LOC     0.9301    0.9406    0.9353      1668
            MISC     0.8089    0.8319    0.8202       702

all (micro avg.)     0.9178    0.9272    0.9225      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9735    0.9759    0.9747      1617
             ORG     0.8971    0.9235    0.9101      1661
             LOC     0.9449    0.9353    0.9400      1668
            MISC     0.8246    0.8305    0.8275       702

all (micro avg.)     0.9237    0.9304    0.9271      5648



----------------------------
BiLSTM-CNN-CRF (no ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.9073610377569482
	macro precision = 0.9025451669268871
	macro recall = 0.9122344192634563 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9548    0.9530    0.9539      1617
             LOC     0.9079    0.9400    0.9237      1668
             ORG     0.8618    0.8934    0.8773      1661
            MISC     0.8138    0.7906    0.8020       702

all (micro avg.)     0.8961    0.9115    0.9037      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9643    0.9518    0.9580      1617
             LOC     0.9179    0.9388    0.9283      1668
             ORG     0.8738    0.9007    0.8870      1661
            MISC     0.8376    0.7934    0.8149       702

all (micro avg.)     0.9083    0.9132    0.9107      5648


----------------------------
BiLSTM-CNN-CRF (no ELMo)
----------------------------
train with validation set
----------------------------

average over 10 folds
	macro f1 = 0.9101551430232169
	macro precision = 0.9063440210138378
	macro recall = 0.9140049575070822 


** Worst ** model scores - 

                  precision    recall  f1-score   support

            MISC     0.8082    0.8105    0.8094       702
             PER     0.9496    0.9561    0.9529      1617
             ORG     0.8774    0.8874    0.8824      1661
             LOC     0.9152    0.9376    0.9263      1668

all (micro avg.)     0.9007    0.9124    0.9065      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

            MISC     0.8205    0.8077    0.8141       702
             PER     0.9617    0.9623    0.9620      1617
             ORG     0.8970    0.8910    0.8940      1661
             LOC     0.9090    0.9400    0.9243      1668

all (micro avg.)     0.9097    0.9155    0.9126      5648


----------------------------
BiLSTM-CNN-CRF (with ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.9229714804031515
	macro precision = 0.9201331319950187
	macro recall = 0.9258321529745043 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9740    0.9716    0.9728      1617
             ORG     0.8737    0.9037    0.8884      1661
             LOC     0.9392    0.9347    0.9369      1668
            MISC     0.8126    0.8091    0.8108       702

all (micro avg.)     0.9137    0.9205    0.9171      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9758    0.9716    0.9737      1617
             ORG     0.8958    0.9157    0.9056      1661
             LOC     0.9380    0.9430    0.9405      1668
            MISC     0.8212    0.8376    0.8293       702

all (micro avg.)     0.9214    0.9301    0.9257      5648



----------------------------
BiLSTM-CNN-CRF (with ELMo)
----------------------------
train with validation set
----------------------------

average over 10 folds
	macro f1 = 0.9267886466836008
	macro precision = 0.9236787695219577
	macro recall = 0.9299220963172804 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             LOC     0.9388    0.9382    0.9385      1668
            MISC     0.8031    0.8191    0.8110       702
             ORG     0.9005    0.9157    0.9081      1661
             PER     0.9770    0.9722    0.9746      1617

all (micro avg.)     0.9211    0.9265    0.9238      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             LOC     0.9304    0.9460    0.9382      1668
            MISC     0.8208    0.8547    0.8374       702
             ORG     0.9200    0.9139    0.9169      1661
             PER     0.9807    0.9740    0.9774      1617

all (micro avg.)     0.9275    0.9333    0.9304      5648


--------------------------------
BiLSTM-CRF-CASING (without ELMo)
--------------------------------

average over 10 folds
	macro f1 = 0.9072633045005467
	macro precision = 0.9057544135162671
	macro recall = 0.9087818696883853 


** Worst ** model scores - 

                  precision    recall  f1-score   support

            MISC     0.8072    0.7991    0.8031       702
             ORG     0.8851    0.8718    0.8784      1661
             LOC     0.9078    0.9388    0.9231      1668
             PER     0.9578    0.9555    0.9567      1617

all (micro avg.)     0.9032    0.9065    0.9048      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

            MISC     0.8321    0.7835    0.8070       702
             ORG     0.8888    0.8850    0.8869      1661
             LOC     0.9158    0.9388    0.9272      1668
             PER     0.9507    0.9666    0.9586      1617

all (micro avg.)     0.9083    0.9117    0.9100      5648


-----------------------------
BiLSTM-CRF-CASING (with ELMo)
-----------------------------

average over 10 folds
	macro f1 = 0.9240220967487194
	macro precision = 0.9235466594908284
	macro recall = 0.9245042492917847 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9698    0.9716    0.9707      1617
            MISC     0.7942    0.8134    0.8037       702
             LOC     0.9313    0.9424    0.9368      1668
             ORG     0.9096    0.9031    0.9063      1661

all (micro avg.)     0.9186    0.9232    0.9209      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             PER     0.9782    0.9722    0.9752      1617
            MISC     0.8364    0.8376    0.8370       702
             LOC     0.9337    0.9376    0.9357      1668
             ORG     0.9114    0.9169    0.9142      1661

all (micro avg.)     0.9277    0.9290    0.9283      5648



----------------------------
BiLSTM-CNN (CASING, no ELMo)
----------------------------

average over 10 folds
	macro f1 = 0.8923685257762012
	macro precision = 0.8813521248524246
	macro recall = 0.9036650141643058 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8260    0.8802    0.8522      1661
             LOC     0.8928    0.9287    0.9104      1668
            MISC     0.8003    0.7707    0.7852       702
             PER     0.9511    0.9493    0.9502      1617

all (micro avg.)     0.8778    0.9007    0.8891      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8357    0.8820    0.8582      1661
             LOC     0.9094    0.9269    0.9181      1668
            MISC     0.7869    0.7892    0.7881       702
             PER     0.9532    0.9573    0.9553      1617

all (micro avg.)     0.8844    0.9053    0.8947      5648


------------------------------
BiLSTM-CNN (CASING, with ELMo)
------------------------------

average over 10 folds
	macro f1 = 0.9166408580816073
	macro precision = 0.9095803428961048
	macro recall = 0.9238137393767705 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8725    0.9019    0.8869      1661
             PER     0.9740    0.9722    0.9731      1617
             LOC     0.9227    0.9376    0.9301      1668
            MISC     0.8006    0.8177    0.8090       702

all (micro avg.)     0.9068    0.9221    0.9144      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8795    0.9139    0.8964      1661
             PER     0.9698    0.9716    0.9707      1617
             LOC     0.9315    0.9376    0.9346      1668
            MISC     0.8155    0.8376    0.8264       702

all (micro avg.)     0.9121    0.9279    0.9200      5648


-------------------------------------------------------
BiLSTM-CNN (CASING, no ELMo, train with validation set)
-------------------------------------------------------

average over 10 folds
	macro f1 = 0.8935373160803783
	macro precision = 0.8831134985078204
	macro recall = 0.9042138810198301 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8491    0.8706    0.8597      1661
             PER     0.9449    0.9549    0.9499      1617
             LOC     0.8851    0.9371    0.9103      1668
            MISC     0.7956    0.7650    0.7800       702

all (micro avg.)     0.8809    0.9012    0.8910      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8515    0.8838    0.8674      1661
             PER     0.9531    0.9542    0.9536      1617
             LOC     0.9049    0.9353    0.9198      1668
            MISC     0.8038    0.7877    0.7957       702

all (micro avg.)     0.8904    0.9072    0.8987      5648



---------------------------------------------------------
BiLSTM-CNN (CASING, with ELMo, train with validation set)
---------------------------------------------------------

average over 10 folds
	macro f1 = 0.9201250430609109
	macro precision = 0.913559893189429
	macro recall = 0.9267882436260624 


** Worst ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8880    0.9067    0.8972      1661
            MISC     0.8044    0.8319    0.8179       702
             PER     0.9734    0.9734    0.9734      1617
             LOC     0.9240    0.9329    0.9284      1668

all (micro avg.)     0.9121    0.9242    0.9181      5648


** Best ** model scores - 

                  precision    recall  f1-score   support

             ORG     0.8788    0.9211    0.8995      1661
            MISC     0.8123    0.8262    0.8192       702
             PER     0.9747    0.9759    0.9753      1617
             LOC     0.9342    0.9371    0.9356      1668

all (micro avg.)     0.9137    0.9297    0.9216      5648



********************************************************
****** Ontonotes 5.0 ***********************************
********************************************************


****** all corpus ***********************************

nb total sentences: 103766
nb total tokens: 2192130
84050 train sequences
	 nb. tokens 1772848
9339 validation sequences
	 nb. tokens 198565
10377 evaluation sequences
	 nb. tokens 220717


82122 train sequences
	 nb. tokens 1633660
	 with nb. entities 239617
12678 validation sequences
	 nb. tokens 249271
	 with nb. entities 37534
8262 evaluation sequences
	 nb. tokens 152728
	 with nb. entities 20913
total distinct characters: 114


------ without ELMo ---------------------------------------

    training runtime: 7969.122 seconds 

    Evaluation on test set:
        f1 (micro): 86.17
                 precision    recall  f1-score   support

       QUANTITY     0.7321    0.7810    0.7558       105
          EVENT     0.6275    0.5079    0.5614        63
           NORP     0.9193    0.9215    0.9204       841
       CARDINAL     0.8294    0.7487    0.7870       935
        ORDINAL     0.7982    0.9128    0.8517       195
            ORG     0.8451    0.8635    0.8542      1795
       LANGUAGE     0.7059    0.5455    0.6154        22
           TIME     0.6000    0.5943    0.5972       212
        PRODUCT     0.7333    0.5789    0.6471        76
            FAC     0.6630    0.4519    0.5374       135
           DATE     0.8015    0.8571    0.8284      1602
          MONEY     0.8714    0.8631    0.8672       314
            LAW     0.6786    0.4750    0.5588        40
        PERCENT     0.8808    0.8682    0.8745       349
    WORK_OF_ART     0.6480    0.4880    0.5567       166
            LOC     0.7500    0.7709    0.7603       179
            GPE     0.9494    0.9388    0.9441      2240
         PERSON     0.9038    0.9306    0.9170      1988

    avg / total     0.8618    0.8615    0.8617     11257


------ with ELMo ------------------------------------------
(usual model 5.5B)

Evaluation on test set:
	f1 (micro): 79.62
             precision    recall  f1-score   support

WORK_OF_ART     0.5510    0.6506    0.5967       166
    PRODUCT     0.6582    0.6842    0.6710        76
      MONEY     0.8116    0.8503    0.8305       314
        FAC     0.7130    0.5704    0.6337       135
   LANGUAGE     0.7778    0.6364    0.7000        22
   QUANTITY     0.1361    0.8000    0.2327       105
       TIME     0.6370    0.4387    0.5196       212
        GPE     0.9535    0.9437    0.9486      2240
      EVENT     0.6316    0.7619    0.6906        63
    PERCENT     0.8499    0.8596    0.8547       349
        ORG     0.9003    0.8758    0.8879      1795
        LOC     0.7611    0.7654    0.7632       179
     PERSON     0.9297    0.9452    0.9374      1988
    ORDINAL     0.8148    0.1128    0.1982       195
        LAW     0.5405    0.5000    0.5195        40
       NORP     0.9191    0.9322    0.9256       841
   CARDINAL     0.8512    0.1102    0.1951       935
       DATE     0.8537    0.5137    0.6415      1602

avg / total     0.8423    0.7548    0.7962     11257



------ with ELMo ------------------------------------------
ELMo first model (not the 5.5B)

average over 10 folds
	macro f1 = 0.7852888179838143
	macro precision = 0.8304988758759713
	macro recall = 0.744754375055521 


** Best ** model scores - 

                  precision    recall  f1-score   support

           MONEY     0.8431    0.8726    0.8576       314
            TIME     0.6692    0.4104    0.5088       212
           EVENT     0.4889    0.6984    0.5752        63
             FAC     0.7736    0.6074    0.6805       135
          PERSON     0.9129    0.9432    0.9278      1988
             GPE     0.9549    0.9545    0.9547      2240
             LOC     0.7391    0.7598    0.7493       179
         ORDINAL     0.8704    0.2410    0.3775       195
     WORK_OF_ART     0.6294    0.6446    0.6369       166
             ORG     0.8883    0.8641    0.8760      1795
        LANGUAGE     0.7500    0.6818    0.7143        22
         PERCENT     0.8097    0.8166    0.8131       349
            DATE     0.8392    0.5181    0.6407      1602
        QUANTITY     0.1434    0.7714    0.2418       105
        CARDINAL     0.7789    0.1658    0.2734       935
            NORP     0.8930    0.8930    0.8930       841
             LAW     0.5000    0.4500    0.4737        40
         PRODUCT     0.6250    0.7237    0.6707        76

all (micro avg.)     0.8374    0.7578    0.7956     11257



