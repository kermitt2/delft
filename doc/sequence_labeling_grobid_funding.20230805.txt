> python3 delft/applications/grobidTagger.py funding-acknowledgement train_eval --architecture BidLSTM_CRF_FEATURES --use-ELMo --input data/sequenceLabelling/grobid/funding-acknowledgement/funding-230729.train --fold-count 10
Loading data...
1163 train sequences
130 validation sequences
144 evaluation sequences

max train sequence length: 468
max validation sequence length: 1119
max evaluation sequence length: 310
ELMo weights used: /media/lopez/T51/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5
ELMo config used: /media/lopez/T51/embeddings/elmo_2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json
Output directory: data/models/sequenceLabelling/grobid-funding-acknowledgement-BidLSTM_CRF_FEATURES-with_ELMo
BidLSTM_CRF_FEATURES
---
max_epoch: 60
early_stop: True
patience: 5
batch_size (training): 30
max_sequence_length: 500
model_name: grobid-funding-acknowledgement-BidLSTM_CRF_FEATURES-with_ELMo
learning_rate:  0.001
use_ELMo:  True
---

** Worst ** model scores - run 7
                  precision    recall  f1-score   support

   <affiliation>     0.6562    0.8750    0.7500        24
    <funderName>     0.6580    0.7922    0.7189       255
     <grantName>     0.4074    0.4231    0.4151        26
   <grantNumber>     0.8057    0.8812    0.8418       160
   <institution>     0.5000    0.4563    0.4772       103
        <person>     0.9624    0.9624    0.9624       266
   <programName>     0.2917    0.3043    0.2979        23
   <projectName>     0.4062    0.3824    0.3939        34

all (micro avg.)     0.7294    0.7834    0.7554       891


** Best ** model scores - run 2
                  precision    recall  f1-score   support

   <affiliation>     0.7241    0.8750    0.7925        24
    <funderName>     0.7445    0.8000    0.7713       255
     <grantName>     0.5172    0.5769    0.5455        26
   <grantNumber>     0.8011    0.8812    0.8393       160
   <institution>     0.6091    0.6505    0.6291       103
        <person>     0.9446    0.9624    0.9534       266
   <programName>     0.2917    0.3043    0.2979        23
   <projectName>     0.5882    0.5882    0.5882        34

all (micro avg.)     0.7719    0.8204    0.7954       891

----------------------------------------------------------------------

Average over 10 folds
                  precision    recall  f1-score   support

   <affiliation>     0.7120    0.8833    0.7878        24
    <funderName>     0.6911    0.8000    0.7411       255
     <grantName>     0.4220    0.4423    0.4309        26
   <grantNumber>     0.8044    0.8781    0.8396       160
   <institution>     0.5717    0.5515    0.5596       103
        <person>     0.9511    0.9643    0.9576       266
   <programName>     0.2970    0.2913    0.2924        23
   <projectName>     0.4887    0.4912    0.4894        34

all (micro avg.)     0.7483    0.8012    0.7739 




> python3 delft/applications/grobidTagger.py funding-acknowledgement train_eval --architecture BidLSTM_CRF_FEATURES  --input data/sequenceLabelling/grobid/funding-acknowledgement/funding-230729.train


  f1 (micro): 75.89
                  precision    recall  f1-score   support

   <affiliation>     0.7000    0.8750    0.7778        24
    <funderName>     0.7165    0.7333    0.7248       255
     <grantName>     0.3636    0.3077    0.3333        26
   <grantNumber>     0.8171    0.8938    0.8537       160
   <institution>     0.4955    0.5340    0.5140       103
        <person>     0.9416    0.9699    0.9556       266
   <programName>     0.2800    0.3043    0.2917        23
   <projectName>     0.3750    0.4412    0.4054        34

all (micro avg.)     0.7399    0.7789    0.7589       891

  f1 (micro): 76.53
                  precision    recall  f1-score   support

   <affiliation>     0.5806    0.7500    0.6545        24
    <funderName>     0.7222    0.7647    0.7429       255
     <grantName>     0.5263    0.3846    0.4444        26
   <grantNumber>     0.8333    0.9062    0.8683       160
   <institution>     0.5000    0.4854    0.4926       103
        <person>     0.9382    0.9699    0.9538       266
   <programName>     0.2083    0.2174    0.2128        23
   <projectName>     0.4286    0.4412    0.4348        34

all (micro avg.)     0.7500    0.7811    0.7653       891
