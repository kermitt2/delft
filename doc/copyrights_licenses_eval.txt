GRU - glove-840B
================

* ensemble 10 classifiers (10 folds) *

> python3 delft/applications/licenseClassifier.py train_eval --embedding glove-840B --fold-count 10

Evaluation on 76 instances:
                   precision        recall       f-score       support
     publisher        0.9667        1.0000        0.9831            29
       authors        1.0000        0.9600        0.9796            25
     undecided        0.9545        0.9545        0.9545            22

Evaluation on 71 instances:
                   precision        recall       f-score       support
          CC-0        0.0000        0.0000        0.0000             0
         CC-BY        1.0000        0.9565        0.9778            23
      CC-BY-NC        1.0000        1.0000        1.0000             3
   CC-BY-NC-ND        1.0000        1.0000        1.0000             8
      CC-BY-SA        1.0000        1.0000        1.0000             5
   CC-BY-NC-SA        1.0000        1.0000        1.0000             2
      CC-BY-ND        1.0000        1.0000        1.0000             1
     copyright        1.0000        1.0000        1.0000             7
         other        0.0000        0.0000        0.0000             0
     undecided        0.9565        1.0000        0.9778            22

* single classifier

> python3 delft/applications/licenseClassifier.py train_eval --embedding glove-840B

Evaluation on 76 instances:
                   precision        recall       f-score       support
     publisher        0.9655        0.9655        0.9655            29
       authors        0.9600        0.9600        0.9600            25
     undecided        0.9545        0.9545        0.9545            22

Evaluation on 86 instances:
                   precision        recall       f-score       support
          CC-0        0.0000        0.0000        0.0000             0
         CC-BY        0.9412        1.0000        0.9697            32
      CC-BY-NC        1.0000        1.0000        1.0000             4
   CC-BY-NC-ND        0.8000        0.8889        0.8421             9
      CC-BY-SA        1.0000        1.0000        1.0000             5
   CC-BY-NC-SA        1.0000        0.7500        0.8571             4
      CC-BY-ND        0.0000        0.0000        0.0000             2
     copyright        0.8571        1.0000        0.9231             6
         other        0.0000        0.0000        0.0000             0
     undecided        1.0000        0.9583        0.9787            24


SciBERT
=======

> python3 delft/applications/licenseClassifier.py train_eval --transformer allenai/scibert_scivocab_cased --architecture bert

Evaluation on 76 instances:
                   precision        recall       f-score       support
     publisher        0.8750        0.9655        0.9180            29
       authors        0.9200        0.9200        0.9200            25
     undecided        1.0000        0.8636        0.9268            22

Evaluation on 83 instances:
                   precision        recall       f-score       support
          CC-0        0.0000        0.0000        0.0000             0
         CC-BY        0.9583        1.0000        0.9787            23
      CC-BY-NC        0.0000        0.0000        0.0000             2
   CC-BY-NC-ND        0.9375        1.0000        0.9677            15
      CC-BY-SA        1.0000        1.0000        1.0000             2
   CC-BY-NC-SA        1.0000        1.0000        1.0000             1
      CC-BY-ND        1.0000        1.0000        1.0000             1
     copyright        1.0000        1.0000        1.0000            10
         other        0.0000        0.0000        0.0000             1
     undecided        0.9655        1.0000        0.9825            28
     
BERT-base-cased
===============

> python3 delft/applications/licenseClassifier.py train_eval --transformer bert-base-cased --architecture bert

Evaluation on 76 instances:
                   precision        recall       f-score       support
     publisher        0.8788        1.0000        0.9355            29
       authors        0.9583        0.9200        0.9388            25
     undecided        1.0000        0.8636        0.9268            22

Evaluation on 83 instances:
                   precision        recall       f-score       support
          CC-0        0.0000        0.0000        0.0000             0
         CC-BY        0.9565        0.9565        0.9565            23
      CC-BY-NC        0.0000        0.0000        0.0000             2
   CC-BY-NC-ND        0.7143        1.0000        0.8333            15
      CC-BY-SA        0.5000        0.5000        0.5000             2
   CC-BY-NC-SA        0.0000        0.0000        0.0000             1
      CC-BY-ND        0.0000        0.0000        0.0000             1
     copyright        1.0000        0.9000        0.9474            10
         other        0.0000        0.0000        0.0000             1
     undecided        0.9643        0.9643        0.9643            28


BioLinkBERT-base
================

> python3 delft/applications/licenseClassifier.py train_eval --transformer michiyasunaga/BioLinkBERT-base --architecture bert

Evaluation on 76 instances:
                   precision        recall       f-score       support
     publisher        0.8788        1.0000        0.9355            29
       authors        1.0000        0.9200        0.9583            25
     undecided        0.9500        0.8636        0.9048            22

Evaluation on 83 instances:
                   precision        recall       f-score       support
          CC-0        0.0000        0.0000        0.0000             0
         CC-BY        0.9167        0.9565        0.9362            23
      CC-BY-NC        0.0000        0.0000        0.0000             2
   CC-BY-NC-ND        0.7500        1.0000        0.8571            15
      CC-BY-SA        1.0000        0.5000        0.6667             2
   CC-BY-NC-SA        0.0000        0.0000        0.0000             1
      CC-BY-ND        0.0000        0.0000        0.0000             1
     copyright        1.0000        0.9000        0.9474            10
         other        0.0000        0.0000        0.0000             1
     undecided        0.9655        1.0000        0.9825            28



